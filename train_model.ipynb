{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 732, Validation batches: 209\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models, transforms\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# If using your custom data loader, import it (or define your own here)\n",
    "from data import build_split_dataloaders  # ...existing code...\n",
    "from model import SwinTransformerClassificationModel  # New import for SwinTransformer\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "experiment = \"swin_attention_weighted\"\n",
    "log_dir = f\"runs/{experiment}\"\n",
    "\n",
    "# Data paths\n",
    "root_dir = os.path.join(\"K:\", \"rsna-breast-cancer-detection\")\n",
    "csv_path = os.path.join(root_dir, \"train.csv\")\n",
    "root_dir = os.path.join(root_dir, \"train_images_cropped\")\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # RESIZE TO 224x224\n",
    "    transforms.Resize((224, 224)),\n",
    "])\n",
    "\n",
    "# Build dataloaders (Assumes build_split_dataloaders is defined in data.py)\n",
    "train_loader, val_loader, test_loader = build_split_dataloaders(\n",
    "    csv_path, root_dir, batch_size=batch_size, transform=transform, train=True, val_ratio=0.2, test_ratio=0.1, paired=True\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    8756\n",
      "1.0    6266\n",
      "2.0    1692\n",
      "Name: count, dtype: int64\n",
      "tensor([0.1321, 0.1845, 0.6834], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from data import MedicalImageDataset\n",
    "\n",
    "# Create a new dataset to check the class distribution\n",
    "dataset = MedicalImageDataset(csv_path, root_dir, transform=transform, paired=True)\n",
    "dataframe = dataset.metadata\n",
    "cc_birads = dataframe[\"cc_birads\"].values\n",
    "mlo_birads = dataframe[\"mlo_birads\"].values\n",
    "\n",
    "birads_counts = cc_birads.tolist() + mlo_birads.tolist()\n",
    "birads_counts = pd.Series(birads_counts).value_counts()\n",
    "\n",
    "print(birads_counts)\n",
    "\n",
    "# Create weights for loss function based on class distribution\n",
    "birads_weights = 1 / birads_counts\n",
    "birads_weights = birads_weights / birads_weights.sum()\n",
    "birads_weights = birads_weights.sort_index()\n",
    "\n",
    "birads_weights = torch.tensor(birads_weights.values).float()\n",
    "birads_weights = birads_weights.to(\"cuda\")\n",
    "\n",
    "print(birads_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from model import SwinTransformerClassificationModel\n",
    "# Initialize model, criterion, optimizer, and TensorBoard writer\n",
    "\n",
    "# Use SwinTransformer instead of resnet18\n",
    "num_classes = 3\n",
    "\n",
    "model = SwinTransformerClassificationModel(num_classes=num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from model import SwinMammoClassifier\n",
    "\n",
    "# Initialize model, criterion, optimizer, and TensorBoard writer\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "model = SwinMammoClassifier(num_classes=num_classes)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 10] loss: 0.480  accuracy: 0.106 total accuracy: 0.106 avg loss: 0.480\n",
      "[Epoch 1, Batch 20] loss: 0.426  accuracy: 0.412 total accuracy: 0.259 avg loss: 0.453\n",
      "[Epoch 1, Batch 30] loss: 0.480  accuracy: 0.281 total accuracy: 0.267 avg loss: 0.462\n",
      "[Epoch 1, Batch 40] loss: 0.467  accuracy: 0.406 total accuracy: 0.302 avg loss: 0.463\n",
      "[Epoch 1, Batch 50] loss: 0.432  accuracy: 0.487 total accuracy: 0.339 avg loss: 0.457\n",
      "[Epoch 1, Batch 60] loss: 0.442  accuracy: 0.412 total accuracy: 0.351 avg loss: 0.455\n",
      "[Epoch 1, Batch 70] loss: 0.516  accuracy: 0.325 total accuracy: 0.347 avg loss: 0.464\n",
      "[Epoch 1, Batch 80] loss: 0.520  accuracy: 0.163 total accuracy: 0.324 avg loss: 0.471\n",
      "[Epoch 1, Batch 90] loss: 0.456  accuracy: 0.188 total accuracy: 0.309 avg loss: 0.469\n",
      "[Epoch 1, Batch 100] loss: 0.375  accuracy: 0.450 total accuracy: 0.323 avg loss: 0.460\n",
      "[Epoch 1, Batch 110] loss: 0.478  accuracy: 0.325 total accuracy: 0.323 avg loss: 0.461\n",
      "[Epoch 1, Batch 120] loss: 0.489  accuracy: 0.175 total accuracy: 0.311 avg loss: 0.464\n",
      "[Epoch 1, Batch 130] loss: 0.433  accuracy: 0.406 total accuracy: 0.318 avg loss: 0.461\n",
      "[Epoch 1, Batch 140] loss: 0.341  accuracy: 0.637 total accuracy: 0.341 avg loss: 0.453\n",
      "[Epoch 1, Batch 150] loss: 0.497  accuracy: 0.287 total accuracy: 0.338 avg loss: 0.456\n",
      "[Epoch 1, Batch 160] loss: 0.444  accuracy: 0.419 total accuracy: 0.343 avg loss: 0.455\n",
      "[Epoch 1, Batch 170] loss: 0.552  accuracy: 0.312 total accuracy: 0.341 avg loss: 0.461\n",
      "[Epoch 1, Batch 180] loss: 0.513  accuracy: 0.150 total accuracy: 0.330 avg loss: 0.463\n",
      "[Epoch 1, Batch 190] loss: 0.509  accuracy: 0.169 total accuracy: 0.322 avg loss: 0.466\n",
      "[Epoch 1, Batch 200] loss: 0.418  accuracy: 0.244 total accuracy: 0.318 avg loss: 0.464\n",
      "[Epoch 1, Batch 210] loss: 0.487  accuracy: 0.356 total accuracy: 0.320 avg loss: 0.465\n",
      "[Epoch 1, Batch 220] loss: 0.480  accuracy: 0.181 total accuracy: 0.313 avg loss: 0.465\n",
      "[Epoch 1, Batch 230] loss: 0.444  accuracy: 0.388 total accuracy: 0.317 avg loss: 0.464\n",
      "[Epoch 1, Batch 240] loss: 0.405  accuracy: 0.294 total accuracy: 0.316 avg loss: 0.462\n",
      "[Epoch 1, Batch 250] loss: 0.432  accuracy: 0.250 total accuracy: 0.313 avg loss: 0.461\n",
      "[Epoch 1, Batch 260] loss: 0.475  accuracy: 0.306 total accuracy: 0.313 avg loss: 0.461\n",
      "[Epoch 1, Batch 270] loss: 0.399  accuracy: 0.463 total accuracy: 0.318 avg loss: 0.459\n",
      "[Epoch 1, Batch 280] loss: 0.522  accuracy: 0.419 total accuracy: 0.322 avg loss: 0.461\n",
      "[Epoch 1, Batch 290] loss: 0.419  accuracy: 0.263 total accuracy: 0.320 avg loss: 0.460\n",
      "[Epoch 1, Batch 300] loss: 0.500  accuracy: 0.294 total accuracy: 0.319 avg loss: 0.461\n",
      "[Epoch 1, Batch 310] loss: 0.473  accuracy: 0.319 total accuracy: 0.319 avg loss: 0.462\n",
      "[Epoch 1, Batch 320] loss: 0.452  accuracy: 0.344 total accuracy: 0.320 avg loss: 0.461\n",
      "[Epoch 1, Batch 330] loss: 0.474  accuracy: 0.244 total accuracy: 0.317 avg loss: 0.462\n",
      "[Epoch 1, Batch 340] loss: 0.431  accuracy: 0.412 total accuracy: 0.320 avg loss: 0.461\n",
      "[Epoch 1, Batch 350] loss: 0.503  accuracy: 0.306 total accuracy: 0.320 avg loss: 0.462\n",
      "[Epoch 1, Batch 360] loss: 0.493  accuracy: 0.150 total accuracy: 0.315 avg loss: 0.463\n",
      "[Epoch 1, Batch 370] loss: 0.446  accuracy: 0.212 total accuracy: 0.312 avg loss: 0.462\n",
      "[Epoch 1, Batch 380] loss: 0.519  accuracy: 0.263 total accuracy: 0.311 avg loss: 0.464\n",
      "[Epoch 1, Batch 390] loss: 0.520  accuracy: 0.181 total accuracy: 0.308 avg loss: 0.465\n",
      "[Epoch 1, Batch 400] loss: 0.484  accuracy: 0.188 total accuracy: 0.305 avg loss: 0.466\n",
      "[Epoch 1, Batch 410] loss: 0.476  accuracy: 0.225 total accuracy: 0.303 avg loss: 0.466\n",
      "[Epoch 1, Batch 420] loss: 0.404  accuracy: 0.588 total accuracy: 0.310 avg loss: 0.465\n",
      "[Epoch 1, Batch 430] loss: 0.368  accuracy: 0.438 total accuracy: 0.312 avg loss: 0.462\n",
      "[Epoch 1, Batch 440] loss: 0.406  accuracy: 0.588 total accuracy: 0.319 avg loss: 0.461\n",
      "[Epoch 1, Batch 450] loss: 0.424  accuracy: 0.362 total accuracy: 0.320 avg loss: 0.460\n",
      "[Epoch 1, Batch 460] loss: 0.465  accuracy: 0.431 total accuracy: 0.322 avg loss: 0.460\n",
      "[Epoch 1, Batch 470] loss: 0.474  accuracy: 0.275 total accuracy: 0.321 avg loss: 0.461\n",
      "[Epoch 1, Batch 480] loss: 0.470  accuracy: 0.256 total accuracy: 0.320 avg loss: 0.461\n",
      "[Epoch 1, Batch 490] loss: 0.381  accuracy: 0.456 total accuracy: 0.323 avg loss: 0.459\n",
      "[Epoch 1, Batch 500] loss: 0.617  accuracy: 0.369 total accuracy: 0.324 avg loss: 0.462\n",
      "[Epoch 1, Batch 510] loss: 0.523  accuracy: 0.237 total accuracy: 0.322 avg loss: 0.463\n",
      "[Epoch 1, Batch 520] loss: 0.471  accuracy: 0.269 total accuracy: 0.321 avg loss: 0.464\n",
      "[Epoch 1, Batch 530] loss: 0.496  accuracy: 0.225 total accuracy: 0.319 avg loss: 0.464\n",
      "[Epoch 1, Batch 540] loss: 0.505  accuracy: 0.156 total accuracy: 0.316 avg loss: 0.465\n",
      "[Epoch 1, Batch 550] loss: 0.524  accuracy: 0.163 total accuracy: 0.313 avg loss: 0.466\n",
      "[Epoch 1, Batch 560] loss: 0.439  accuracy: 0.350 total accuracy: 0.314 avg loss: 0.466\n",
      "[Epoch 1, Batch 570] loss: 0.473  accuracy: 0.375 total accuracy: 0.315 avg loss: 0.466\n",
      "[Epoch 1, Batch 580] loss: 0.462  accuracy: 0.194 total accuracy: 0.313 avg loss: 0.466\n",
      "[Epoch 1, Batch 590] loss: 0.434  accuracy: 0.350 total accuracy: 0.313 avg loss: 0.465\n",
      "[Epoch 1, Batch 600] loss: 0.420  accuracy: 0.562 total accuracy: 0.318 avg loss: 0.464\n",
      "[Epoch 1, Batch 610] loss: 0.432  accuracy: 0.562 total accuracy: 0.322 avg loss: 0.464\n",
      "[Epoch 1, Batch 620] loss: 0.418  accuracy: 0.519 total accuracy: 0.325 avg loss: 0.463\n",
      "[Epoch 1, Batch 630] loss: 0.517  accuracy: 0.312 total accuracy: 0.325 avg loss: 0.464\n",
      "[Epoch 1, Batch 640] loss: 0.469  accuracy: 0.144 total accuracy: 0.322 avg loss: 0.464\n",
      "[Epoch 1, Batch 650] loss: 0.408  accuracy: 0.463 total accuracy: 0.324 avg loss: 0.463\n",
      "[Epoch 1, Batch 660] loss: 0.344  accuracy: 0.444 total accuracy: 0.326 avg loss: 0.461\n",
      "[Epoch 1, Batch 670] loss: 0.512  accuracy: 0.338 total accuracy: 0.326 avg loss: 0.462\n",
      "[Epoch 1, Batch 680] loss: 0.463  accuracy: 0.263 total accuracy: 0.325 avg loss: 0.462\n",
      "[Epoch 1, Batch 690] loss: 0.517  accuracy: 0.338 total accuracy: 0.325 avg loss: 0.463\n",
      "[Epoch 1, Batch 700] loss: 0.506  accuracy: 0.269 total accuracy: 0.324 avg loss: 0.464\n",
      "[Epoch 1, Batch 710] loss: 0.589  accuracy: 0.263 total accuracy: 0.324 avg loss: 0.465\n",
      "[Epoch 1, Batch 720] loss: 0.432  accuracy: 0.194 total accuracy: 0.322 avg loss: 0.465\n",
      "[Epoch 1, Batch 730] loss: 0.490  accuracy: 0.381 total accuracy: 0.323 avg loss: 0.465\n",
      "Epoch 1 completed in 712.70 seconds\n",
      "Validation loss after epoch 1: 0.000  accuracy: 0.105\n",
      "[Epoch 2, Batch 10] loss: 0.423  accuracy: 0.406 total accuracy: 0.406 avg loss: 0.423\n",
      "[Epoch 2, Batch 20] loss: 0.490  accuracy: 0.306 total accuracy: 0.356 avg loss: 0.457\n",
      "[Epoch 2, Batch 30] loss: 0.473  accuracy: 0.325 total accuracy: 0.346 avg loss: 0.462\n",
      "[Epoch 2, Batch 40] loss: 0.484  accuracy: 0.331 total accuracy: 0.342 avg loss: 0.468\n",
      "[Epoch 2, Batch 50] loss: 0.492  accuracy: 0.106 total accuracy: 0.295 avg loss: 0.473\n",
      "[Epoch 2, Batch 60] loss: 0.540  accuracy: 0.225 total accuracy: 0.283 avg loss: 0.484\n",
      "[Epoch 2, Batch 70] loss: 0.514  accuracy: 0.156 total accuracy: 0.265 avg loss: 0.488\n",
      "[Epoch 2, Batch 80] loss: 0.467  accuracy: 0.375 total accuracy: 0.279 avg loss: 0.485\n",
      "[Epoch 2, Batch 90] loss: 0.456  accuracy: 0.244 total accuracy: 0.275 avg loss: 0.482\n",
      "[Epoch 2, Batch 100] loss: 0.484  accuracy: 0.219 total accuracy: 0.269 avg loss: 0.482\n",
      "[Epoch 2, Batch 110] loss: 0.435  accuracy: 0.225 total accuracy: 0.265 avg loss: 0.478\n",
      "[Epoch 2, Batch 120] loss: 0.440  accuracy: 0.431 total accuracy: 0.279 avg loss: 0.475\n",
      "[Epoch 2, Batch 130] loss: 0.441  accuracy: 0.537 total accuracy: 0.299 avg loss: 0.472\n",
      "[Epoch 2, Batch 140] loss: 0.511  accuracy: 0.256 total accuracy: 0.296 avg loss: 0.475\n",
      "[Epoch 2, Batch 150] loss: 0.500  accuracy: 0.131 total accuracy: 0.285 avg loss: 0.477\n",
      "[Epoch 2, Batch 160] loss: 0.491  accuracy: 0.163 total accuracy: 0.277 avg loss: 0.478\n",
      "[Epoch 2, Batch 170] loss: 0.479  accuracy: 0.369 total accuracy: 0.283 avg loss: 0.478\n",
      "[Epoch 2, Batch 180] loss: 0.436  accuracy: 0.263 total accuracy: 0.282 avg loss: 0.475\n",
      "[Epoch 2, Batch 190] loss: 0.504  accuracy: 0.219 total accuracy: 0.278 avg loss: 0.477\n",
      "[Epoch 2, Batch 200] loss: 0.442  accuracy: 0.331 total accuracy: 0.281 avg loss: 0.475\n",
      "[Epoch 2, Batch 210] loss: 0.541  accuracy: 0.225 total accuracy: 0.278 avg loss: 0.478\n",
      "[Epoch 2, Batch 220] loss: 0.466  accuracy: 0.338 total accuracy: 0.281 avg loss: 0.478\n",
      "[Epoch 2, Batch 230] loss: 0.421  accuracy: 0.481 total accuracy: 0.290 avg loss: 0.475\n",
      "[Epoch 2, Batch 240] loss: 0.488  accuracy: 0.419 total accuracy: 0.295 avg loss: 0.476\n",
      "[Epoch 2, Batch 250] loss: 0.432  accuracy: 0.438 total accuracy: 0.301 avg loss: 0.474\n",
      "[Epoch 2, Batch 260] loss: 0.486  accuracy: 0.281 total accuracy: 0.300 avg loss: 0.474\n",
      "[Epoch 2, Batch 270] loss: 0.479  accuracy: 0.219 total accuracy: 0.297 avg loss: 0.475\n",
      "[Epoch 2, Batch 280] loss: 0.448  accuracy: 0.244 total accuracy: 0.295 avg loss: 0.474\n",
      "[Epoch 2, Batch 290] loss: 0.436  accuracy: 0.350 total accuracy: 0.297 avg loss: 0.472\n",
      "[Epoch 2, Batch 300] loss: 0.447  accuracy: 0.481 total accuracy: 0.303 avg loss: 0.472\n",
      "[Epoch 2, Batch 310] loss: 0.384  accuracy: 0.463 total accuracy: 0.308 avg loss: 0.469\n",
      "[Epoch 2, Batch 320] loss: 0.444  accuracy: 0.444 total accuracy: 0.312 avg loss: 0.468\n",
      "[Epoch 2, Batch 330] loss: 0.472  accuracy: 0.431 total accuracy: 0.316 avg loss: 0.468\n",
      "[Epoch 2, Batch 340] loss: 0.444  accuracy: 0.562 total accuracy: 0.323 avg loss: 0.467\n",
      "[Epoch 2, Batch 350] loss: 0.342  accuracy: 0.544 total accuracy: 0.330 avg loss: 0.464\n",
      "[Epoch 2, Batch 360] loss: 0.448  accuracy: 0.450 total accuracy: 0.333 avg loss: 0.463\n",
      "[Epoch 2, Batch 370] loss: 0.420  accuracy: 0.463 total accuracy: 0.336 avg loss: 0.462\n",
      "[Epoch 2, Batch 380] loss: 0.555  accuracy: 0.194 total accuracy: 0.333 avg loss: 0.465\n",
      "[Epoch 2, Batch 390] loss: 0.443  accuracy: 0.350 total accuracy: 0.333 avg loss: 0.464\n",
      "[Epoch 2, Batch 400] loss: 0.454  accuracy: 0.306 total accuracy: 0.333 avg loss: 0.464\n",
      "[Epoch 2, Batch 410] loss: 0.448  accuracy: 0.419 total accuracy: 0.335 avg loss: 0.463\n",
      "[Epoch 2, Batch 420] loss: 0.505  accuracy: 0.181 total accuracy: 0.331 avg loss: 0.464\n",
      "[Epoch 2, Batch 430] loss: 0.493  accuracy: 0.312 total accuracy: 0.331 avg loss: 0.465\n",
      "[Epoch 2, Batch 440] loss: 0.468  accuracy: 0.250 total accuracy: 0.329 avg loss: 0.465\n",
      "[Epoch 2, Batch 450] loss: 0.514  accuracy: 0.263 total accuracy: 0.327 avg loss: 0.466\n",
      "[Epoch 2, Batch 460] loss: 0.394  accuracy: 0.206 total accuracy: 0.325 avg loss: 0.465\n",
      "[Epoch 2, Batch 470] loss: 0.470  accuracy: 0.469 total accuracy: 0.328 avg loss: 0.465\n",
      "[Epoch 2, Batch 480] loss: 0.457  accuracy: 0.388 total accuracy: 0.329 avg loss: 0.465\n",
      "[Epoch 2, Batch 490] loss: 2.218  accuracy: 0.350 total accuracy: 0.329 avg loss: 0.500\n",
      "[Epoch 2, Batch 500] loss: 16.198  accuracy: 0.388 total accuracy: 0.331 avg loss: 0.814\n",
      "[Epoch 2, Batch 510] loss: 56.457  accuracy: 0.362 total accuracy: 0.331 avg loss: 1.905\n",
      "[Epoch 2, Batch 520] loss: 51.122  accuracy: 0.312 total accuracy: 0.331 avg loss: 2.852\n",
      "[Epoch 2, Batch 530] loss: 48.022  accuracy: 0.287 total accuracy: 0.330 avg loss: 3.704\n",
      "[Epoch 2, Batch 540] loss: 19.851  accuracy: 0.388 total accuracy: 0.331 avg loss: 4.003\n",
      "[Epoch 2, Batch 550] loss: 10.537  accuracy: 0.300 total accuracy: 0.330 avg loss: 4.122\n",
      "[Epoch 2, Batch 560] loss: 6.217  accuracy: 0.300 total accuracy: 0.330 avg loss: 4.159\n",
      "[Epoch 2, Batch 570] loss: 5.371  accuracy: 0.338 total accuracy: 0.330 avg loss: 4.181\n",
      "[Epoch 2, Batch 580] loss: 2.974  accuracy: 0.312 total accuracy: 0.330 avg loss: 4.160\n",
      "[Epoch 2, Batch 590] loss: 1.630  accuracy: 0.312 total accuracy: 0.329 avg loss: 4.117\n",
      "[Epoch 2, Batch 600] loss: 0.878  accuracy: 0.381 total accuracy: 0.330 avg loss: 4.063\n",
      "[Epoch 2, Batch 610] loss: 1.131  accuracy: 0.356 total accuracy: 0.331 avg loss: 4.015\n",
      "[Epoch 2, Batch 620] loss: 3.447  accuracy: 0.319 total accuracy: 0.331 avg loss: 4.006\n",
      "[Epoch 2, Batch 630] loss: 2.582  accuracy: 0.500 total accuracy: 0.333 avg loss: 3.983\n",
      "[Epoch 2, Batch 640] loss: 1.806  accuracy: 0.350 total accuracy: 0.333 avg loss: 3.949\n",
      "[Epoch 2, Batch 650] loss: 0.806  accuracy: 0.300 total accuracy: 0.333 avg loss: 3.901\n",
      "[Epoch 2, Batch 660] loss: 0.744  accuracy: 0.231 total accuracy: 0.331 avg loss: 3.853\n",
      "[Epoch 2, Batch 670] loss: 0.761  accuracy: 0.394 total accuracy: 0.332 avg loss: 3.807\n",
      "[Epoch 2, Batch 680] loss: 0.603  accuracy: 0.331 total accuracy: 0.332 avg loss: 3.760\n",
      "[Epoch 2, Batch 690] loss: 0.544  accuracy: 0.194 total accuracy: 0.330 avg loss: 3.713\n",
      "[Epoch 2, Batch 700] loss: 0.597  accuracy: 0.331 total accuracy: 0.330 avg loss: 3.669\n",
      "[Epoch 2, Batch 710] loss: 0.527  accuracy: 0.287 total accuracy: 0.330 avg loss: 3.624\n",
      "[Epoch 2, Batch 720] loss: 0.536  accuracy: 0.306 total accuracy: 0.329 avg loss: 3.581\n",
      "[Epoch 2, Batch 730] loss: 0.489  accuracy: 0.356 total accuracy: 0.330 avg loss: 3.539\n",
      "Epoch 2 completed in 653.12 seconds\n",
      "Validation loss after epoch 2: 0.000  accuracy: 0.318\n",
      "[Epoch 3, Batch 10] loss: 0.694  accuracy: 0.281 total accuracy: 0.281 avg loss: 0.694\n",
      "[Epoch 3, Batch 20] loss: 0.546  accuracy: 0.362 total accuracy: 0.322 avg loss: 0.620\n",
      "[Epoch 3, Batch 30] loss: 0.879  accuracy: 0.338 total accuracy: 0.327 avg loss: 0.706\n",
      "[Epoch 3, Batch 40] loss: 0.593  accuracy: 0.256 total accuracy: 0.309 avg loss: 0.678\n",
      "[Epoch 3, Batch 50] loss: 0.600  accuracy: 0.287 total accuracy: 0.305 avg loss: 0.662\n",
      "[Epoch 3, Batch 60] loss: 0.844  accuracy: 0.138 total accuracy: 0.277 avg loss: 0.693\n",
      "[Epoch 3, Batch 70] loss: 0.503  accuracy: 0.312 total accuracy: 0.282 avg loss: 0.666\n",
      "[Epoch 3, Batch 80] loss: 0.628  accuracy: 0.431 total accuracy: 0.301 avg loss: 0.661\n",
      "[Epoch 3, Batch 90] loss: 0.675  accuracy: 0.212 total accuracy: 0.291 avg loss: 0.663\n",
      "[Epoch 3, Batch 100] loss: 0.445  accuracy: 0.338 total accuracy: 0.296 avg loss: 0.641\n",
      "[Epoch 3, Batch 110] loss: 0.533  accuracy: 0.469 total accuracy: 0.311 avg loss: 0.631\n",
      "[Epoch 3, Batch 120] loss: 0.670  accuracy: 0.312 total accuracy: 0.311 avg loss: 0.634\n",
      "[Epoch 3, Batch 130] loss: 0.471  accuracy: 0.331 total accuracy: 0.313 avg loss: 0.622\n",
      "[Epoch 3, Batch 140] loss: 0.616  accuracy: 0.275 total accuracy: 0.310 avg loss: 0.621\n",
      "[Epoch 3, Batch 150] loss: 0.639  accuracy: 0.225 total accuracy: 0.305 avg loss: 0.622\n",
      "[Epoch 3, Batch 160] loss: 0.537  accuracy: 0.375 total accuracy: 0.309 avg loss: 0.617\n",
      "[Epoch 3, Batch 170] loss: 0.617  accuracy: 0.294 total accuracy: 0.308 avg loss: 0.617\n",
      "[Epoch 3, Batch 180] loss: 0.485  accuracy: 0.306 total accuracy: 0.308 avg loss: 0.610\n",
      "[Epoch 3, Batch 190] loss: 0.526  accuracy: 0.444 total accuracy: 0.315 avg loss: 0.605\n",
      "[Epoch 3, Batch 200] loss: 0.561  accuracy: 0.350 total accuracy: 0.317 avg loss: 0.603\n",
      "[Epoch 3, Batch 210] loss: 0.513  accuracy: 0.306 total accuracy: 0.316 avg loss: 0.599\n",
      "[Epoch 3, Batch 220] loss: 0.666  accuracy: 0.306 total accuracy: 0.316 avg loss: 0.602\n",
      "[Epoch 3, Batch 230] loss: 0.572  accuracy: 0.294 total accuracy: 0.315 avg loss: 0.601\n",
      "[Epoch 3, Batch 240] loss: 0.572  accuracy: 0.269 total accuracy: 0.313 avg loss: 0.599\n",
      "[Epoch 3, Batch 250] loss: 0.648  accuracy: 0.237 total accuracy: 0.310 avg loss: 0.601\n",
      "[Epoch 3, Batch 260] loss: 0.584  accuracy: 0.188 total accuracy: 0.305 avg loss: 0.601\n",
      "[Epoch 3, Batch 270] loss: 0.639  accuracy: 0.300 total accuracy: 0.305 avg loss: 0.602\n",
      "[Epoch 3, Batch 280] loss: 0.550  accuracy: 0.144 total accuracy: 0.299 avg loss: 0.600\n",
      "[Epoch 3, Batch 290] loss: 0.641  accuracy: 0.481 total accuracy: 0.306 avg loss: 0.602\n",
      "[Epoch 3, Batch 300] loss: 0.498  accuracy: 0.144 total accuracy: 0.300 avg loss: 0.598\n",
      "[Epoch 3, Batch 310] loss: 0.515  accuracy: 0.294 total accuracy: 0.300 avg loss: 0.596\n",
      "[Epoch 3, Batch 320] loss: 0.498  accuracy: 0.419 total accuracy: 0.304 avg loss: 0.592\n",
      "[Epoch 3, Batch 330] loss: 0.518  accuracy: 0.350 total accuracy: 0.305 avg loss: 0.590\n",
      "[Epoch 3, Batch 340] loss: 0.426  accuracy: 0.294 total accuracy: 0.305 avg loss: 0.585\n",
      "[Epoch 3, Batch 350] loss: 0.577  accuracy: 0.475 total accuracy: 0.310 avg loss: 0.585\n",
      "[Epoch 3, Batch 360] loss: 0.582  accuracy: 0.263 total accuracy: 0.308 avg loss: 0.585\n",
      "[Epoch 3, Batch 370] loss: 0.550  accuracy: 0.362 total accuracy: 0.310 avg loss: 0.584\n",
      "[Epoch 3, Batch 380] loss: 0.407  accuracy: 0.394 total accuracy: 0.312 avg loss: 0.579\n",
      "[Epoch 3, Batch 390] loss: 0.395  accuracy: 0.425 total accuracy: 0.315 avg loss: 0.575\n",
      "[Epoch 3, Batch 400] loss: 0.430  accuracy: 0.394 total accuracy: 0.317 avg loss: 0.571\n",
      "[Epoch 3, Batch 410] loss: 0.545  accuracy: 0.338 total accuracy: 0.317 avg loss: 0.570\n",
      "[Epoch 3, Batch 420] loss: 0.424  accuracy: 0.369 total accuracy: 0.319 avg loss: 0.567\n",
      "[Epoch 3, Batch 430] loss: 0.560  accuracy: 0.369 total accuracy: 0.320 avg loss: 0.567\n",
      "[Epoch 3, Batch 440] loss: 0.494  accuracy: 0.369 total accuracy: 0.321 avg loss: 0.565\n",
      "[Epoch 3, Batch 450] loss: 0.563  accuracy: 0.294 total accuracy: 0.320 avg loss: 0.565\n",
      "[Epoch 3, Batch 460] loss: 0.526  accuracy: 0.250 total accuracy: 0.319 avg loss: 0.564\n",
      "[Epoch 3, Batch 470] loss: 0.636  accuracy: 0.294 total accuracy: 0.318 avg loss: 0.566\n",
      "[Epoch 3, Batch 480] loss: 0.461  accuracy: 0.356 total accuracy: 0.319 avg loss: 0.564\n",
      "[Epoch 3, Batch 490] loss: 0.519  accuracy: 0.362 total accuracy: 0.320 avg loss: 0.563\n",
      "[Epoch 3, Batch 500] loss: 0.747  accuracy: 0.256 total accuracy: 0.319 avg loss: 0.566\n",
      "[Epoch 3, Batch 510] loss: 0.516  accuracy: 0.344 total accuracy: 0.319 avg loss: 0.565\n",
      "[Epoch 3, Batch 520] loss: 0.492  accuracy: 0.406 total accuracy: 0.321 avg loss: 0.564\n",
      "[Epoch 3, Batch 530] loss: 0.558  accuracy: 0.188 total accuracy: 0.318 avg loss: 0.564\n",
      "[Epoch 3, Batch 540] loss: 0.719  accuracy: 0.469 total accuracy: 0.321 avg loss: 0.567\n",
      "[Epoch 3, Batch 550] loss: 0.465  accuracy: 0.225 total accuracy: 0.319 avg loss: 0.565\n",
      "[Epoch 3, Batch 560] loss: 0.535  accuracy: 0.531 total accuracy: 0.323 avg loss: 0.564\n",
      "[Epoch 3, Batch 570] loss: 0.491  accuracy: 0.406 total accuracy: 0.325 avg loss: 0.563\n",
      "[Epoch 3, Batch 580] loss: 0.474  accuracy: 0.388 total accuracy: 0.326 avg loss: 0.562\n",
      "[Epoch 3, Batch 590] loss: 0.543  accuracy: 0.325 total accuracy: 0.326 avg loss: 0.561\n",
      "[Epoch 3, Batch 600] loss: 0.502  accuracy: 0.219 total accuracy: 0.324 avg loss: 0.560\n",
      "[Epoch 3, Batch 610] loss: 0.467  accuracy: 0.319 total accuracy: 0.324 avg loss: 0.559\n",
      "[Epoch 3, Batch 620] loss: 0.501  accuracy: 0.431 total accuracy: 0.326 avg loss: 0.558\n",
      "[Epoch 3, Batch 630] loss: 0.550  accuracy: 0.300 total accuracy: 0.325 avg loss: 0.558\n",
      "[Epoch 3, Batch 640] loss: 0.640  accuracy: 0.219 total accuracy: 0.323 avg loss: 0.559\n",
      "[Epoch 3, Batch 650] loss: 0.478  accuracy: 0.319 total accuracy: 0.323 avg loss: 0.558\n",
      "[Epoch 3, Batch 660] loss: 0.422  accuracy: 0.388 total accuracy: 0.324 avg loss: 0.556\n",
      "[Epoch 3, Batch 670] loss: 0.626  accuracy: 0.331 total accuracy: 0.324 avg loss: 0.557\n",
      "[Epoch 3, Batch 680] loss: 0.465  accuracy: 0.400 total accuracy: 0.326 avg loss: 0.555\n",
      "[Epoch 3, Batch 690] loss: 0.630  accuracy: 0.438 total accuracy: 0.327 avg loss: 0.556\n",
      "[Epoch 3, Batch 700] loss: 0.586  accuracy: 0.231 total accuracy: 0.326 avg loss: 0.557\n",
      "[Epoch 3, Batch 710] loss: 0.443  accuracy: 0.263 total accuracy: 0.325 avg loss: 0.555\n",
      "[Epoch 3, Batch 720] loss: 0.509  accuracy: 0.356 total accuracy: 0.325 avg loss: 0.555\n",
      "[Epoch 3, Batch 730] loss: 0.888  accuracy: 0.338 total accuracy: 0.326 avg loss: 0.559\n",
      "Epoch 3 completed in 655.06 seconds\n",
      "Validation loss after epoch 3: 0.000  accuracy: 0.105\n",
      "[Epoch 4, Batch 10] loss: 0.530  accuracy: 0.431 total accuracy: 0.431 avg loss: 0.530\n",
      "[Epoch 4, Batch 20] loss: 0.748  accuracy: 0.388 total accuracy: 0.409 avg loss: 0.639\n",
      "[Epoch 4, Batch 30] loss: 0.429  accuracy: 0.275 total accuracy: 0.365 avg loss: 0.569\n",
      "[Epoch 4, Batch 40] loss: 0.785  accuracy: 0.419 total accuracy: 0.378 avg loss: 0.623\n",
      "[Epoch 4, Batch 50] loss: 0.697  accuracy: 0.169 total accuracy: 0.336 avg loss: 0.638\n",
      "[Epoch 4, Batch 60] loss: 0.504  accuracy: 0.263 total accuracy: 0.324 avg loss: 0.616\n",
      "[Epoch 4, Batch 70] loss: 0.576  accuracy: 0.231 total accuracy: 0.311 avg loss: 0.610\n",
      "[Epoch 4, Batch 80] loss: 0.565  accuracy: 0.212 total accuracy: 0.298 avg loss: 0.604\n",
      "[Epoch 4, Batch 90] loss: 0.488  accuracy: 0.206 total accuracy: 0.288 avg loss: 0.591\n",
      "[Epoch 4, Batch 100] loss: 0.600  accuracy: 0.312 total accuracy: 0.291 avg loss: 0.592\n",
      "[Epoch 4, Batch 110] loss: 0.463  accuracy: 0.438 total accuracy: 0.304 avg loss: 0.581\n",
      "[Epoch 4, Batch 120] loss: 0.514  accuracy: 0.269 total accuracy: 0.301 avg loss: 0.575\n",
      "[Epoch 4, Batch 130] loss: 0.519  accuracy: 0.344 total accuracy: 0.304 avg loss: 0.571\n",
      "[Epoch 4, Batch 140] loss: 0.471  accuracy: 0.450 total accuracy: 0.315 avg loss: 0.564\n",
      "[Epoch 4, Batch 150] loss: 0.411  accuracy: 0.319 total accuracy: 0.315 avg loss: 0.553\n",
      "[Epoch 4, Batch 160] loss: 0.514  accuracy: 0.425 total accuracy: 0.322 avg loss: 0.551\n",
      "[Epoch 4, Batch 170] loss: 0.466  accuracy: 0.419 total accuracy: 0.328 avg loss: 0.546\n",
      "[Epoch 4, Batch 180] loss: 0.547  accuracy: 0.231 total accuracy: 0.322 avg loss: 0.546\n",
      "[Epoch 4, Batch 190] loss: 0.650  accuracy: 0.369 total accuracy: 0.325 avg loss: 0.551\n",
      "[Epoch 4, Batch 200] loss: 0.563  accuracy: 0.181 total accuracy: 0.318 avg loss: 0.552\n",
      "[Epoch 4, Batch 210] loss: 0.563  accuracy: 0.275 total accuracy: 0.315 avg loss: 0.553\n",
      "[Epoch 4, Batch 220] loss: 0.586  accuracy: 0.237 total accuracy: 0.312 avg loss: 0.554\n",
      "[Epoch 4, Batch 230] loss: 0.562  accuracy: 0.194 total accuracy: 0.307 avg loss: 0.554\n",
      "[Epoch 4, Batch 240] loss: 0.509  accuracy: 0.338 total accuracy: 0.308 avg loss: 0.553\n",
      "[Epoch 4, Batch 250] loss: 0.729  accuracy: 0.319 total accuracy: 0.308 avg loss: 0.560\n",
      "[Epoch 4, Batch 260] loss: 0.462  accuracy: 0.425 total accuracy: 0.313 avg loss: 0.556\n",
      "[Epoch 4, Batch 270] loss: 0.715  accuracy: 0.275 total accuracy: 0.312 avg loss: 0.562\n",
      "[Epoch 4, Batch 280] loss: 0.527  accuracy: 0.350 total accuracy: 0.313 avg loss: 0.561\n",
      "[Epoch 4, Batch 290] loss: 0.473  accuracy: 0.369 total accuracy: 0.315 avg loss: 0.558\n",
      "[Epoch 4, Batch 300] loss: 0.588  accuracy: 0.450 total accuracy: 0.319 avg loss: 0.559\n",
      "[Epoch 4, Batch 310] loss: 0.598  accuracy: 0.275 total accuracy: 0.318 avg loss: 0.560\n",
      "[Epoch 4, Batch 320] loss: 0.510  accuracy: 0.188 total accuracy: 0.314 avg loss: 0.558\n",
      "[Epoch 4, Batch 330] loss: 0.740  accuracy: 0.338 total accuracy: 0.315 avg loss: 0.564\n",
      "[Epoch 4, Batch 340] loss: 0.506  accuracy: 0.225 total accuracy: 0.312 avg loss: 0.562\n",
      "[Epoch 4, Batch 350] loss: 0.494  accuracy: 0.331 total accuracy: 0.312 avg loss: 0.560\n",
      "[Epoch 4, Batch 360] loss: 1.010  accuracy: 0.350 total accuracy: 0.314 avg loss: 0.573\n",
      "[Epoch 4, Batch 370] loss: 0.588  accuracy: 0.244 total accuracy: 0.312 avg loss: 0.573\n",
      "[Epoch 4, Batch 380] loss: 0.509  accuracy: 0.369 total accuracy: 0.313 avg loss: 0.571\n",
      "[Epoch 4, Batch 390] loss: 0.506  accuracy: 0.475 total accuracy: 0.317 avg loss: 0.570\n",
      "[Epoch 4, Batch 400] loss: 0.578  accuracy: 0.181 total accuracy: 0.314 avg loss: 0.570\n",
      "[Epoch 4, Batch 410] loss: 0.513  accuracy: 0.244 total accuracy: 0.312 avg loss: 0.569\n",
      "[Epoch 4, Batch 420] loss: 0.420  accuracy: 0.506 total accuracy: 0.317 avg loss: 0.565\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 44\u001b[0m\n\u001b[0;32m     40\u001b[0m weigted_cc_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(cc_loss \u001b[38;5;241m*\u001b[39m birads_weights[cc_labels])\n\u001b[0;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m weigted_mlo_loss \u001b[38;5;241m+\u001b[39m weigted_cc_loss\n\u001b[1;32m---> 44\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     47\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mj:\\AI4Health\\.venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mj:\\AI4Health\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mj:\\AI4Health\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Training loop with extra logging\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train_batch = 0\n",
    "    correct_total = 0\n",
    "    total_train_batch = 0\n",
    "    total_train = 0\n",
    "    total_loss = 0.0\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        mlo, cc = inputs\n",
    "        mlo_labels, cc_labels = labels\n",
    "        mlo = mlo.to(device)\n",
    "        cc = cc.to(device)\n",
    "        mlo_labels = mlo_labels.to(device).long()\n",
    "        cc_labels = cc_labels.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mlo_outputs, cc_outputs = model(mlo, cc)\n",
    "        \n",
    "        # Ensure the batch sizes match\n",
    "        mlo_outputs = mlo_outputs.view(-1, num_classes)\n",
    "        cc_outputs = cc_outputs.view(-1, num_classes)\n",
    "        mlo_labels = mlo_labels.view(-1)\n",
    "        cc_labels = cc_labels.view(-1)\n",
    "        \n",
    "        loss = criterion(mlo_outputs, mlo_labels) + criterion(cc_outputs, cc_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        # Compute predictions\n",
    "        _, mlo_preds = torch.max(mlo_outputs, 1)\n",
    "        _, cc_preds = torch.max(cc_outputs, 1)\n",
    "        \n",
    "        correct_train_batch += torch.sum(mlo_preds == mlo_labels).item() + torch.sum(cc_preds == cc_labels).item()\n",
    "        \n",
    "        \n",
    "        total_train_batch += mlo_labels.size(0) + cc_labels.size(0)\n",
    "        global_step += 1\n",
    "        \n",
    "        # _, preds = torch.max(outputs, 1)\n",
    "        # correct_train += torch.sum(preds == labels).item()\n",
    "        # total_train += labels.size(0)\n",
    "        # global_step += 1\n",
    "        \n",
    "        if i % 10 == 9:\n",
    "            avg_loss = running_loss / 10\n",
    "            \n",
    "            total_train += total_train_batch\n",
    "            correct_total += correct_train_batch\n",
    "            total_loss += running_loss\n",
    "            \n",
    "            avg_total_loss = total_loss / (i+1)\n",
    "            \n",
    "            train_acc_batch = correct_train_batch / total_train_batch\n",
    "            print(f\"[Epoch {epoch+1}, Batch {i+1}] loss: {avg_loss:.3f}  accuracy: {train_acc_batch:.3f} total accuracy: {correct_total / total_train:.3f} avg loss: {avg_total_loss:.3f}\")\n",
    "            writer.add_scalar('training loss', avg_loss, global_step)\n",
    "            writer.add_scalar('training accuracy', train_acc_batch, global_step)\n",
    "            \n",
    "            writer.add_scalar('total training accuracy', correct_total / total_train, global_step)\n",
    "            writer.add_scalar('total training loss', avg_total_loss, global_step)\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            correct_train_batch = 0\n",
    "            total_train_batch = 0\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds\")\n",
    "    \n",
    "    # Validation loop with accuracy logging\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            mlo, cc = inputs\n",
    "            mlo_labels, cc_labels = labels\n",
    "            mlo = mlo.to(device)\n",
    "            cc = cc.to(device)\n",
    "            mlo_labels = mlo_labels.to(device).long()\n",
    "            cc_labels = cc_labels.to(device).long()\n",
    "            \n",
    "            mlo_outputs, cc_outputs = model(mlo, cc)\n",
    "            \n",
    "            # Ensure the batch sizes match\n",
    "            mlo_outputs = mlo_outputs.view(-1, num_classes)\n",
    "            cc_outputs = cc_outputs.view(-1, num_classes)\n",
    "            mlo_labels = mlo_labels.view(-1)\n",
    "            cc_labels = cc_labels.view(-1)\n",
    "            \n",
    "            loss = criterion(mlo_outputs, mlo_labels) + criterion(cc_outputs, cc_labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, mlo_preds = torch.max(mlo_outputs, 1)\n",
    "            _, cc_preds = torch.max(cc_outputs, 1)\n",
    "            correct_val += torch.sum(mlo_preds == mlo_labels).item() + torch.sum(cc_preds == cc_labels).item()\n",
    "            total_val += mlo_labels.size(0) + cc_labels.size(0)\n",
    "    val_loss_avg = val_loss / len(val_loader)\n",
    "    val_acc = correct_val / total_val\n",
    "    print(f\"Validation loss after epoch {epoch+1}: {val_loss_avg:.3f}  accuracy: {val_acc:.3f}\")\n",
    "    writer.add_scalar('validation loss', val_loss_avg, epoch)\n",
    "    writer.add_scalar('validation accuracy', val_acc, epoch)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), f\"model_{experiment}.pth\")\n",
    "print(\"Model saved to model.pth\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
