{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 2301, Validation batches: 658\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models, transforms\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# If using your custom data loader, import it (or define your own here)\n",
    "from data import build_split_dataloaders  # ...existing code...\n",
    "from model import SwinTransformerClassificationModel  # New import for SwinTransformer\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "log_dir = \"runs/experiment_2\"\n",
    "\n",
    "# Data paths\n",
    "root_dir = os.path.join(\"K:\", \"rsna-breast-cancer-detection\")\n",
    "csv_path = os.path.join(root_dir, \"train.csv\")\n",
    "root_dir = os.path.join(root_dir, \"train_images_cropped\")\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # RESIZE TO 224x224\n",
    "    transforms.Resize((224, 224)),\n",
    "])\n",
    "\n",
    "# Build dataloaders (Assumes build_split_dataloaders is defined in data.py)\n",
    "train_loader, val_loader, test_loader = build_split_dataloaders(\n",
    "    csv_path, root_dir, batch_size=batch_size, transform=transform, train=True, val_ratio=0.2, test_ratio=0.1\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, criterion, optimizer, and TensorBoard writer\n",
    "\n",
    "# Use SwinTransformer instead of resnet18\n",
    "num_classes = 3\n",
    "\n",
    "model = timm.create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=3, global_pool='avg')\n",
    "        \n",
    "old_proj = model.patch_embed.proj\n",
    "model.patch_embed.proj = nn.Conv2d(\n",
    "    in_channels=1,\n",
    "    out_channels=old_proj.out_channels,\n",
    "    kernel_size=old_proj.kernel_size,\n",
    "    stride=old_proj.stride,\n",
    "    padding=old_proj.padding,\n",
    "    bias=old_proj.bias is not None\n",
    ")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 10] loss: 2.028  accuracy: 0.537\n",
      "[Epoch 1, Batch 20] loss: 1.118  accuracy: 0.512\n",
      "[Epoch 1, Batch 30] loss: 0.971  accuracy: 0.512\n",
      "[Epoch 1, Batch 40] loss: 0.811  accuracy: 0.675\n",
      "[Epoch 1, Batch 50] loss: 0.969  accuracy: 0.463\n",
      "[Epoch 1, Batch 60] loss: 0.947  accuracy: 0.625\n",
      "[Epoch 1, Batch 70] loss: 1.000  accuracy: 0.450\n",
      "[Epoch 1, Batch 80] loss: 0.907  accuracy: 0.625\n",
      "[Epoch 1, Batch 90] loss: 0.977  accuracy: 0.487\n",
      "[Epoch 1, Batch 100] loss: 1.031  accuracy: 0.562\n",
      "[Epoch 1, Batch 110] loss: 0.919  accuracy: 0.550\n",
      "[Epoch 1, Batch 120] loss: 0.927  accuracy: 0.625\n",
      "[Epoch 1, Batch 130] loss: 1.022  accuracy: 0.287\n",
      "[Epoch 1, Batch 140] loss: 0.909  accuracy: 0.600\n",
      "[Epoch 1, Batch 150] loss: 0.990  accuracy: 0.525\n",
      "[Epoch 1, Batch 160] loss: 0.941  accuracy: 0.537\n",
      "[Epoch 1, Batch 170] loss: 0.951  accuracy: 0.575\n",
      "[Epoch 1, Batch 180] loss: 0.955  accuracy: 0.562\n",
      "[Epoch 1, Batch 190] loss: 0.944  accuracy: 0.562\n",
      "[Epoch 1, Batch 200] loss: 0.922  accuracy: 0.537\n",
      "[Epoch 1, Batch 210] loss: 0.825  accuracy: 0.650\n",
      "[Epoch 1, Batch 220] loss: 0.861  accuracy: 0.688\n",
      "[Epoch 1, Batch 230] loss: 0.913  accuracy: 0.600\n",
      "[Epoch 1, Batch 240] loss: 0.880  accuracy: 0.588\n",
      "[Epoch 1, Batch 250] loss: 0.968  accuracy: 0.550\n",
      "[Epoch 1, Batch 260] loss: 0.828  accuracy: 0.637\n",
      "[Epoch 1, Batch 270] loss: 1.056  accuracy: 0.500\n",
      "[Epoch 1, Batch 280] loss: 0.926  accuracy: 0.475\n",
      "[Epoch 1, Batch 290] loss: 0.972  accuracy: 0.512\n",
      "[Epoch 1, Batch 300] loss: 0.937  accuracy: 0.588\n",
      "[Epoch 1, Batch 310] loss: 0.980  accuracy: 0.537\n",
      "[Epoch 1, Batch 320] loss: 0.911  accuracy: 0.525\n",
      "[Epoch 1, Batch 330] loss: 1.012  accuracy: 0.512\n",
      "[Epoch 1, Batch 340] loss: 0.942  accuracy: 0.500\n",
      "[Epoch 1, Batch 350] loss: 0.873  accuracy: 0.637\n",
      "[Epoch 1, Batch 360] loss: 0.813  accuracy: 0.688\n",
      "[Epoch 1, Batch 370] loss: 0.845  accuracy: 0.662\n",
      "[Epoch 1, Batch 380] loss: 0.920  accuracy: 0.625\n",
      "[Epoch 1, Batch 390] loss: 0.791  accuracy: 0.700\n",
      "[Epoch 1, Batch 400] loss: 0.866  accuracy: 0.613\n",
      "[Epoch 1, Batch 410] loss: 0.896  accuracy: 0.637\n",
      "[Epoch 1, Batch 420] loss: 0.876  accuracy: 0.550\n",
      "[Epoch 1, Batch 430] loss: 0.873  accuracy: 0.588\n",
      "[Epoch 1, Batch 440] loss: 0.884  accuracy: 0.613\n",
      "[Epoch 1, Batch 450] loss: 0.760  accuracy: 0.775\n",
      "[Epoch 1, Batch 460] loss: 1.013  accuracy: 0.525\n",
      "[Epoch 1, Batch 470] loss: 0.839  accuracy: 0.625\n",
      "[Epoch 1, Batch 480] loss: 0.969  accuracy: 0.613\n",
      "[Epoch 1, Batch 490] loss: 0.821  accuracy: 0.662\n",
      "[Epoch 1, Batch 500] loss: 0.888  accuracy: 0.562\n",
      "[Epoch 1, Batch 510] loss: 0.850  accuracy: 0.637\n",
      "[Epoch 1, Batch 520] loss: 0.831  accuracy: 0.625\n",
      "[Epoch 1, Batch 530] loss: 1.024  accuracy: 0.425\n",
      "[Epoch 1, Batch 540] loss: 0.893  accuracy: 0.438\n",
      "[Epoch 1, Batch 550] loss: 0.901  accuracy: 0.637\n",
      "[Epoch 1, Batch 560] loss: 0.917  accuracy: 0.588\n",
      "[Epoch 1, Batch 570] loss: 0.900  accuracy: 0.588\n",
      "[Epoch 1, Batch 580] loss: 0.926  accuracy: 0.550\n",
      "[Epoch 1, Batch 590] loss: 0.885  accuracy: 0.613\n",
      "[Epoch 1, Batch 600] loss: 0.852  accuracy: 0.625\n",
      "[Epoch 1, Batch 610] loss: 0.953  accuracy: 0.537\n",
      "[Epoch 1, Batch 620] loss: 0.974  accuracy: 0.562\n",
      "[Epoch 1, Batch 630] loss: 0.773  accuracy: 0.675\n",
      "[Epoch 1, Batch 640] loss: 1.124  accuracy: 0.500\n",
      "[Epoch 1, Batch 650] loss: 0.968  accuracy: 0.550\n",
      "[Epoch 1, Batch 660] loss: 1.012  accuracy: 0.525\n",
      "[Epoch 1, Batch 670] loss: 0.998  accuracy: 0.475\n",
      "[Epoch 1, Batch 680] loss: 0.897  accuracy: 0.562\n",
      "[Epoch 1, Batch 690] loss: 0.873  accuracy: 0.613\n",
      "[Epoch 1, Batch 700] loss: 0.793  accuracy: 0.688\n",
      "[Epoch 1, Batch 710] loss: 0.918  accuracy: 0.613\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 23\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Compute predictions\u001b[39;00m\n\u001b[0;32m     25\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop with extra logging\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    epoch_start = time.time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # Apply pooling if model output is spatial (e.g. [batch, channels, H, W])\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        # Compute predictions\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_train += torch.sum(preds == labels).item()\n",
    "        total_train += labels.size(0)\n",
    "        global_step += 1\n",
    "        \n",
    "        if i % 10 == 9:\n",
    "            avg_loss = running_loss / 10\n",
    "            train_acc = correct_train / total_train\n",
    "            print(f\"[Epoch {epoch+1}, Batch {i+1}] loss: {avg_loss:.3f}  accuracy: {train_acc:.3f}\")\n",
    "            writer.add_scalar('training loss', avg_loss, global_step)\n",
    "            writer.add_scalar('training accuracy', train_acc, global_step)\n",
    "            running_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds\")\n",
    "    \n",
    "    # Validation loop with accuracy logging\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_val += torch.sum(preds == labels).item()\n",
    "            total_val += labels.size(0)\n",
    "    val_loss_avg = val_loss / len(val_loader)\n",
    "    val_acc = correct_val / total_val\n",
    "    print(f\"Validation loss after epoch {epoch+1}: {val_loss_avg:.3f}  accuracy: {val_acc:.3f}\")\n",
    "    writer.add_scalar('validation loss', val_loss_avg, epoch)\n",
    "    writer.add_scalar('validation accuracy', val_acc, epoch)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Model saved to model.pth\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
